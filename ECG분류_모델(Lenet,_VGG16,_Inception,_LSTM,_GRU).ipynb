{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvJcUvweU_YE",
        "outputId": "f087db29-6e88-486f-cd2d-9048f9ae755f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# 구글드라이브와 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글드라이브에 디렉토리 생성\n",
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_KQqfjzdfLk",
        "outputId": "aeb5e0e6-c3ef-4996-af93-4e7f244b1883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/gdrive/MyDrive/이창대_kaggle/대회/2023ECGChallenge\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2cdEe0Id3mU",
        "outputId": "7a78977b-69d0-424c-fa2d-35067cf3ef51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/이창대_kaggle/대회/2023ECGChallenge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 불러오기"
      ],
      "metadata": {
        "id": "XHItwXGVVxU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "xEqh93qcVT6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.read_csv('x_train_lead_3.csv', index_col= 0)"
      ],
      "metadata": {
        "id": "fBfWPNCrx1Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = pd.read_csv('y_train_lead_3.csv', index_col= 0)"
      ],
      "metadata": {
        "id": "1-rc0uLhx-oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MjTGGIcJz9H1",
        "outputId": "052e0bcf-75af-48f7-fe09-603a31f2b057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0         1         2         3         4         5         6  \\\n",
              "0      0.004027  0.007036  0.013299  0.015566  0.011837  0.017615  0.033897   \n",
              "1      0.129462  0.178032  0.181306  0.181784  0.179215  0.175101  0.171440   \n",
              "2      0.068481 -0.005178  0.001190  0.003588  0.019518 -0.002518 -0.090518   \n",
              "3     -0.113197 -0.082623 -0.085593 -0.100356 -0.124412 -0.129758 -0.118144   \n",
              "4      0.016029  0.111345  0.112072  0.114214  0.118272  0.111750  0.094900   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "17074  0.025571  0.047056  0.037535  0.025009  0.013477  0.007690 -0.009603   \n",
              "17075 -0.000861  0.037823  0.023000  0.013171  0.024336  0.013245 -0.014102   \n",
              "17076  0.025079  0.037562  0.038795  0.030777  0.002008 -0.029510 -0.041280   \n",
              "17077 -0.067118 -0.005454  0.006459  0.011369  0.019529  0.043936  0.061342   \n",
              "17078  0.023933  0.020694  0.042705  0.060216  0.066728  0.064990  0.059252   \n",
              "\n",
              "              7         8         9  ...       990       991       992  \\\n",
              "0      0.038186  0.028481  0.015782  ...  0.039724  0.026254  0.022988   \n",
              "1      0.170733  0.173480  0.176682  ...  0.128340  0.101031  0.073476   \n",
              "2     -0.168731 -0.129403 -0.037034  ... -0.114935 -0.130845 -0.152311   \n",
              "3     -0.102316 -0.089775 -0.084767  ...  0.496701  0.089038 -0.239238   \n",
              "4      0.078724  0.063226  0.061908  ...  0.182017  0.193415  0.188348   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "17074 -0.030651 -0.035954 -0.035263  ...  0.011708  0.002955 -0.024055   \n",
              "17075 -0.035705 -0.055064 -0.066929  ...  0.101464  0.074953  0.058442   \n",
              "17076 -0.043300 -0.048570 -0.050841  ... -0.040310 -0.037138 -0.034967   \n",
              "17077  0.065245  0.058397  0.033796  ... -0.008413 -0.022654 -0.015895   \n",
              "17078  0.042515  0.010778 -0.019459  ...  0.158467  0.164481  0.161637   \n",
              "\n",
              "            993       994       995       996       997       998       999  \n",
              "0      0.007176 -0.020181 -0.027834 -0.023032 -0.025277 -0.026068  0.029845  \n",
              "1      0.126178  0.381389  0.614611  0.531848  0.387601  0.369373  0.261665  \n",
              "2     -0.169083 -0.175659 -0.178289 -0.179471 -0.184953 -0.201236 -0.179318  \n",
              "3     -0.348874 -0.314869 -0.278219 -0.265922 -0.268476 -0.273130 -0.175379  \n",
              "4      0.174815  0.159563  0.142341  0.129147  0.127478  0.131084  0.041962  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "17074 -0.041571 -0.038094 -0.031122 -0.026907 -0.035949 -0.064247 -0.060301  \n",
              "17075  0.023930 -0.004083  0.010904  0.018390  0.011376  0.012111  0.012345  \n",
              "17076 -0.034295 -0.034375 -0.034705 -0.073035 -0.178367 -0.257449 -0.196283  \n",
              "17077  0.031364  0.026873 -0.009867  0.009892  0.048402  0.067412  0.055422  \n",
              "17078  0.146680  0.116107  0.081914  0.050846  0.031150  0.030321  0.033640  \n",
              "\n",
              "[17079 rows x 1000 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1047b8db-0dfd-4ca0-affe-a152659d61b1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>990</th>\n",
              "      <th>991</th>\n",
              "      <th>992</th>\n",
              "      <th>993</th>\n",
              "      <th>994</th>\n",
              "      <th>995</th>\n",
              "      <th>996</th>\n",
              "      <th>997</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.004027</td>\n",
              "      <td>0.007036</td>\n",
              "      <td>0.013299</td>\n",
              "      <td>0.015566</td>\n",
              "      <td>0.011837</td>\n",
              "      <td>0.017615</td>\n",
              "      <td>0.033897</td>\n",
              "      <td>0.038186</td>\n",
              "      <td>0.028481</td>\n",
              "      <td>0.015782</td>\n",
              "      <td>...</td>\n",
              "      <td>0.039724</td>\n",
              "      <td>0.026254</td>\n",
              "      <td>0.022988</td>\n",
              "      <td>0.007176</td>\n",
              "      <td>-0.020181</td>\n",
              "      <td>-0.027834</td>\n",
              "      <td>-0.023032</td>\n",
              "      <td>-0.025277</td>\n",
              "      <td>-0.026068</td>\n",
              "      <td>0.029845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.129462</td>\n",
              "      <td>0.178032</td>\n",
              "      <td>0.181306</td>\n",
              "      <td>0.181784</td>\n",
              "      <td>0.179215</td>\n",
              "      <td>0.175101</td>\n",
              "      <td>0.171440</td>\n",
              "      <td>0.170733</td>\n",
              "      <td>0.173480</td>\n",
              "      <td>0.176682</td>\n",
              "      <td>...</td>\n",
              "      <td>0.128340</td>\n",
              "      <td>0.101031</td>\n",
              "      <td>0.073476</td>\n",
              "      <td>0.126178</td>\n",
              "      <td>0.381389</td>\n",
              "      <td>0.614611</td>\n",
              "      <td>0.531848</td>\n",
              "      <td>0.387601</td>\n",
              "      <td>0.369373</td>\n",
              "      <td>0.261665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.068481</td>\n",
              "      <td>-0.005178</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.003588</td>\n",
              "      <td>0.019518</td>\n",
              "      <td>-0.002518</td>\n",
              "      <td>-0.090518</td>\n",
              "      <td>-0.168731</td>\n",
              "      <td>-0.129403</td>\n",
              "      <td>-0.037034</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.114935</td>\n",
              "      <td>-0.130845</td>\n",
              "      <td>-0.152311</td>\n",
              "      <td>-0.169083</td>\n",
              "      <td>-0.175659</td>\n",
              "      <td>-0.178289</td>\n",
              "      <td>-0.179471</td>\n",
              "      <td>-0.184953</td>\n",
              "      <td>-0.201236</td>\n",
              "      <td>-0.179318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.113197</td>\n",
              "      <td>-0.082623</td>\n",
              "      <td>-0.085593</td>\n",
              "      <td>-0.100356</td>\n",
              "      <td>-0.124412</td>\n",
              "      <td>-0.129758</td>\n",
              "      <td>-0.118144</td>\n",
              "      <td>-0.102316</td>\n",
              "      <td>-0.089775</td>\n",
              "      <td>-0.084767</td>\n",
              "      <td>...</td>\n",
              "      <td>0.496701</td>\n",
              "      <td>0.089038</td>\n",
              "      <td>-0.239238</td>\n",
              "      <td>-0.348874</td>\n",
              "      <td>-0.314869</td>\n",
              "      <td>-0.278219</td>\n",
              "      <td>-0.265922</td>\n",
              "      <td>-0.268476</td>\n",
              "      <td>-0.273130</td>\n",
              "      <td>-0.175379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.016029</td>\n",
              "      <td>0.111345</td>\n",
              "      <td>0.112072</td>\n",
              "      <td>0.114214</td>\n",
              "      <td>0.118272</td>\n",
              "      <td>0.111750</td>\n",
              "      <td>0.094900</td>\n",
              "      <td>0.078724</td>\n",
              "      <td>0.063226</td>\n",
              "      <td>0.061908</td>\n",
              "      <td>...</td>\n",
              "      <td>0.182017</td>\n",
              "      <td>0.193415</td>\n",
              "      <td>0.188348</td>\n",
              "      <td>0.174815</td>\n",
              "      <td>0.159563</td>\n",
              "      <td>0.142341</td>\n",
              "      <td>0.129147</td>\n",
              "      <td>0.127478</td>\n",
              "      <td>0.131084</td>\n",
              "      <td>0.041962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17074</th>\n",
              "      <td>0.025571</td>\n",
              "      <td>0.047056</td>\n",
              "      <td>0.037535</td>\n",
              "      <td>0.025009</td>\n",
              "      <td>0.013477</td>\n",
              "      <td>0.007690</td>\n",
              "      <td>-0.009603</td>\n",
              "      <td>-0.030651</td>\n",
              "      <td>-0.035954</td>\n",
              "      <td>-0.035263</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011708</td>\n",
              "      <td>0.002955</td>\n",
              "      <td>-0.024055</td>\n",
              "      <td>-0.041571</td>\n",
              "      <td>-0.038094</td>\n",
              "      <td>-0.031122</td>\n",
              "      <td>-0.026907</td>\n",
              "      <td>-0.035949</td>\n",
              "      <td>-0.064247</td>\n",
              "      <td>-0.060301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17075</th>\n",
              "      <td>-0.000861</td>\n",
              "      <td>0.037823</td>\n",
              "      <td>0.023000</td>\n",
              "      <td>0.013171</td>\n",
              "      <td>0.024336</td>\n",
              "      <td>0.013245</td>\n",
              "      <td>-0.014102</td>\n",
              "      <td>-0.035705</td>\n",
              "      <td>-0.055064</td>\n",
              "      <td>-0.066929</td>\n",
              "      <td>...</td>\n",
              "      <td>0.101464</td>\n",
              "      <td>0.074953</td>\n",
              "      <td>0.058442</td>\n",
              "      <td>0.023930</td>\n",
              "      <td>-0.004083</td>\n",
              "      <td>0.010904</td>\n",
              "      <td>0.018390</td>\n",
              "      <td>0.011376</td>\n",
              "      <td>0.012111</td>\n",
              "      <td>0.012345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17076</th>\n",
              "      <td>0.025079</td>\n",
              "      <td>0.037562</td>\n",
              "      <td>0.038795</td>\n",
              "      <td>0.030777</td>\n",
              "      <td>0.002008</td>\n",
              "      <td>-0.029510</td>\n",
              "      <td>-0.041280</td>\n",
              "      <td>-0.043300</td>\n",
              "      <td>-0.048570</td>\n",
              "      <td>-0.050841</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040310</td>\n",
              "      <td>-0.037138</td>\n",
              "      <td>-0.034967</td>\n",
              "      <td>-0.034295</td>\n",
              "      <td>-0.034375</td>\n",
              "      <td>-0.034705</td>\n",
              "      <td>-0.073035</td>\n",
              "      <td>-0.178367</td>\n",
              "      <td>-0.257449</td>\n",
              "      <td>-0.196283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17077</th>\n",
              "      <td>-0.067118</td>\n",
              "      <td>-0.005454</td>\n",
              "      <td>0.006459</td>\n",
              "      <td>0.011369</td>\n",
              "      <td>0.019529</td>\n",
              "      <td>0.043936</td>\n",
              "      <td>0.061342</td>\n",
              "      <td>0.065245</td>\n",
              "      <td>0.058397</td>\n",
              "      <td>0.033796</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008413</td>\n",
              "      <td>-0.022654</td>\n",
              "      <td>-0.015895</td>\n",
              "      <td>0.031364</td>\n",
              "      <td>0.026873</td>\n",
              "      <td>-0.009867</td>\n",
              "      <td>0.009892</td>\n",
              "      <td>0.048402</td>\n",
              "      <td>0.067412</td>\n",
              "      <td>0.055422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17078</th>\n",
              "      <td>0.023933</td>\n",
              "      <td>0.020694</td>\n",
              "      <td>0.042705</td>\n",
              "      <td>0.060216</td>\n",
              "      <td>0.066728</td>\n",
              "      <td>0.064990</td>\n",
              "      <td>0.059252</td>\n",
              "      <td>0.042515</td>\n",
              "      <td>0.010778</td>\n",
              "      <td>-0.019459</td>\n",
              "      <td>...</td>\n",
              "      <td>0.158467</td>\n",
              "      <td>0.164481</td>\n",
              "      <td>0.161637</td>\n",
              "      <td>0.146680</td>\n",
              "      <td>0.116107</td>\n",
              "      <td>0.081914</td>\n",
              "      <td>0.050846</td>\n",
              "      <td>0.031150</td>\n",
              "      <td>0.030321</td>\n",
              "      <td>0.033640</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17079 rows × 1000 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1047b8db-0dfd-4ca0-affe-a152659d61b1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1047b8db-0dfd-4ca0-affe-a152659d61b1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1047b8db-0dfd-4ca0-affe-a152659d61b1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1이 만성 0이 급성\n",
        "y_train = np.array(y_train['not_accute'])\n",
        "y_train = y_train.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "KhSCUA7dz-Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 형성"
      ],
      "metadata": {
        "id": "nJvMj1cJoVBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au20BJsro1IK",
        "outputId": "81079f4d-e4de-494c-ef94-837f952be7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17079, 1000), (17079, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense, Dropout, Activation, BatchNormalization, Add\n",
        "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPool1D, ZeroPadding1D, LSTM, Bidirectional\n",
        "from keras.models import Sequential, Model\n",
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy import optimize\n",
        "from scipy.io import loadmat\n",
        "import os"
      ],
      "metadata": {
        "id": "MGE1cTsxeyC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lenet"
      ],
      "metadata": {
        "id": "s2LXVsSSodZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_5_model=Sequential()\n",
        "\n",
        "lenet_5_model.add(Conv1D(filters=6, kernel_size=3, padding='same', input_shape=(1000,1)))\n",
        "lenet_5_model.add(BatchNormalization())\n",
        "lenet_5_model.add(Activation('relu'))\n",
        "lenet_5_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n",
        "\n",
        "lenet_5_model.add(Conv1D(filters=16, strides=1, kernel_size=5))\n",
        "lenet_5_model.add(BatchNormalization())\n",
        "lenet_5_model.add(Activation('relu'))\n",
        "lenet_5_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n",
        "\n",
        "lenet_5_model.add(GlobalAveragePooling1D())\n",
        "\n",
        "lenet_5_model.add(Dense(64, activation='relu'))\n",
        "\n",
        "lenet_5_model.add(Dense(32, activation='relu'))\n",
        "\n",
        "lenet_5_model.add(Dense(1, activation = 'sigmoid'))"
      ],
      "metadata": {
        "id": "aaPT7f5hoZcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import math\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    # floor 내림  ex) math.floor(3.5) ->3\n",
        "    # 0.1^(epoch 10의 배수마다 내림)\n",
        "    # 10의 배수마다 0.1에 제곱한 값을 0.001에 곱해 learning rate를 낮춤\n",
        "    lr = 0.001 * math.pow(0.1, math.floor(epoch / 10))\n",
        "    return lr\n",
        "\n",
        "print('예시')\n",
        "print(lr_schedule(9), lr_schedule(10), lr_schedule(19), lr_schedule(20))\n",
        "\n",
        "# lr_scheduler에 집어넣기\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV_ke1h_gJZV",
        "outputId": "25b14ed2-39e5-412a-a1c4-e916f9561d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예시\n",
            "0.001 0.0001 0.0001 1.0000000000000003e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad\n",
        "\n",
        "# 모델 컴파일\n",
        "lenet_5_model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "MsOVe2B4fh6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "filepath = \"best_model_resnet50.h5\"\n",
        "# early_stopping 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
        "# 체크포인트 설정\n",
        "model_checkpoint = ModelCheckpoint(filepath, save_best_only=True, save_weights_only=False, monitor='val_loss', mode='min')"
      ],
      "metadata": {
        "id": "NAnz0QNbfiJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu 사용\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "\n",
        "with tf.device(device_name):\n",
        "    history = lenet_5_model.fit(x=X_train,\n",
        "                          y=y_train,           \n",
        "                          validation_split= 0.1,\n",
        "                          batch_size=128,         \n",
        "                          epochs=200,            \n",
        "                          verbose=2,\n",
        "                        callbacks=[lr_scheduler, early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzFWUWjRfiN6",
        "outputId": "151753dd-c463-4392-99f5-e8d94ee7b566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n",
            "Epoch 1/200\n",
            "121/121 - 13s - loss: 0.6429 - accuracy: 0.6264 - val_loss: 0.7167 - val_accuracy: 0.5117 - lr: 0.0010 - 13s/epoch - 108ms/step\n",
            "Epoch 2/200\n",
            "121/121 - 1s - loss: 0.6180 - accuracy: 0.6581 - val_loss: 0.7763 - val_accuracy: 0.5117 - lr: 0.0010 - 638ms/epoch - 5ms/step\n",
            "Epoch 3/200\n",
            "121/121 - 1s - loss: 0.6085 - accuracy: 0.6746 - val_loss: 0.7848 - val_accuracy: 0.5211 - lr: 0.0010 - 601ms/epoch - 5ms/step\n",
            "Epoch 4/200\n",
            "121/121 - 1s - loss: 0.6029 - accuracy: 0.6799 - val_loss: 0.6982 - val_accuracy: 0.5744 - lr: 0.0010 - 663ms/epoch - 5ms/step\n",
            "Epoch 5/200\n",
            "121/121 - 1s - loss: 0.5988 - accuracy: 0.6864 - val_loss: 0.6908 - val_accuracy: 0.5896 - lr: 0.0010 - 621ms/epoch - 5ms/step\n",
            "Epoch 6/200\n",
            "121/121 - 1s - loss: 0.5954 - accuracy: 0.6859 - val_loss: 0.6432 - val_accuracy: 0.6358 - lr: 0.0010 - 610ms/epoch - 5ms/step\n",
            "Epoch 7/200\n",
            "121/121 - 1s - loss: 0.5953 - accuracy: 0.6920 - val_loss: 0.9165 - val_accuracy: 0.5351 - lr: 0.0010 - 575ms/epoch - 5ms/step\n",
            "Epoch 8/200\n",
            "121/121 - 1s - loss: 0.5907 - accuracy: 0.6902 - val_loss: 0.6501 - val_accuracy: 0.6335 - lr: 0.0010 - 568ms/epoch - 5ms/step\n",
            "Epoch 9/200\n",
            "121/121 - 1s - loss: 0.5878 - accuracy: 0.6931 - val_loss: 0.6375 - val_accuracy: 0.6382 - lr: 0.0010 - 614ms/epoch - 5ms/step\n",
            "Epoch 10/200\n",
            "121/121 - 1s - loss: 0.5898 - accuracy: 0.6934 - val_loss: 0.6921 - val_accuracy: 0.5978 - lr: 0.0010 - 566ms/epoch - 5ms/step\n",
            "Epoch 11/200\n",
            "121/121 - 1s - loss: 0.5830 - accuracy: 0.6975 - val_loss: 0.6626 - val_accuracy: 0.6259 - lr: 1.0000e-04 - 583ms/epoch - 5ms/step\n",
            "Epoch 12/200\n",
            "121/121 - 1s - loss: 0.5827 - accuracy: 0.6987 - val_loss: 0.6447 - val_accuracy: 0.6393 - lr: 1.0000e-04 - 575ms/epoch - 5ms/step\n",
            "Epoch 13/200\n",
            "121/121 - 1s - loss: 0.5826 - accuracy: 0.6957 - val_loss: 0.6398 - val_accuracy: 0.6434 - lr: 1.0000e-04 - 568ms/epoch - 5ms/step\n",
            "Epoch 14/200\n",
            "121/121 - 1s - loss: 0.5818 - accuracy: 0.6964 - val_loss: 0.6296 - val_accuracy: 0.6552 - lr: 1.0000e-04 - 635ms/epoch - 5ms/step\n",
            "Epoch 15/200\n",
            "121/121 - 1s - loss: 0.5816 - accuracy: 0.6977 - val_loss: 0.6415 - val_accuracy: 0.6411 - lr: 1.0000e-04 - 645ms/epoch - 5ms/step\n",
            "Epoch 16/200\n",
            "121/121 - 1s - loss: 0.5815 - accuracy: 0.6997 - val_loss: 0.6404 - val_accuracy: 0.6393 - lr: 1.0000e-04 - 577ms/epoch - 5ms/step\n",
            "Epoch 17/200\n",
            "121/121 - 1s - loss: 0.5817 - accuracy: 0.6980 - val_loss: 0.6400 - val_accuracy: 0.6458 - lr: 1.0000e-04 - 643ms/epoch - 5ms/step\n",
            "Epoch 18/200\n",
            "121/121 - 1s - loss: 0.5809 - accuracy: 0.6965 - val_loss: 0.6385 - val_accuracy: 0.6470 - lr: 1.0000e-04 - 576ms/epoch - 5ms/step\n",
            "Epoch 19/200\n",
            "121/121 - 1s - loss: 0.5804 - accuracy: 0.6979 - val_loss: 0.6254 - val_accuracy: 0.6534 - lr: 1.0000e-04 - 621ms/epoch - 5ms/step\n",
            "Epoch 20/200\n",
            "121/121 - 1s - loss: 0.5804 - accuracy: 0.6985 - val_loss: 0.6542 - val_accuracy: 0.6364 - lr: 1.0000e-04 - 576ms/epoch - 5ms/step\n",
            "Epoch 21/200\n",
            "121/121 - 1s - loss: 0.5805 - accuracy: 0.6992 - val_loss: 0.6379 - val_accuracy: 0.6399 - lr: 1.0000e-05 - 649ms/epoch - 5ms/step\n",
            "Epoch 22/200\n",
            "121/121 - 1s - loss: 0.5811 - accuracy: 0.6971 - val_loss: 0.6370 - val_accuracy: 0.6475 - lr: 1.0000e-05 - 633ms/epoch - 5ms/step\n",
            "Epoch 23/200\n",
            "121/121 - 1s - loss: 0.5799 - accuracy: 0.6971 - val_loss: 0.6392 - val_accuracy: 0.6399 - lr: 1.0000e-05 - 649ms/epoch - 5ms/step\n",
            "Epoch 24/200\n",
            "121/121 - 1s - loss: 0.5806 - accuracy: 0.6988 - val_loss: 0.6379 - val_accuracy: 0.6470 - lr: 1.0000e-05 - 636ms/epoch - 5ms/step\n",
            "Epoch 25/200\n",
            "121/121 - 1s - loss: 0.5792 - accuracy: 0.6986 - val_loss: 0.6382 - val_accuracy: 0.6481 - lr: 1.0000e-05 - 578ms/epoch - 5ms/step\n",
            "Epoch 26/200\n",
            "121/121 - 1s - loss: 0.5800 - accuracy: 0.6973 - val_loss: 0.6400 - val_accuracy: 0.6434 - lr: 1.0000e-05 - 582ms/epoch - 5ms/step\n",
            "Epoch 27/200\n",
            "121/121 - 1s - loss: 0.5793 - accuracy: 0.6996 - val_loss: 0.6389 - val_accuracy: 0.6464 - lr: 1.0000e-05 - 577ms/epoch - 5ms/step\n",
            "Epoch 28/200\n",
            "121/121 - 1s - loss: 0.5799 - accuracy: 0.6978 - val_loss: 0.6405 - val_accuracy: 0.6417 - lr: 1.0000e-05 - 582ms/epoch - 5ms/step\n",
            "Epoch 29/200\n",
            "121/121 - 1s - loss: 0.5795 - accuracy: 0.6980 - val_loss: 0.6380 - val_accuracy: 0.6481 - lr: 1.0000e-05 - 593ms/epoch - 5ms/step\n",
            "Epoch 30/200\n",
            "121/121 - 1s - loss: 0.5800 - accuracy: 0.6979 - val_loss: 0.6398 - val_accuracy: 0.6434 - lr: 1.0000e-05 - 579ms/epoch - 5ms/step\n",
            "Epoch 31/200\n",
            "121/121 - 1s - loss: 0.5790 - accuracy: 0.6983 - val_loss: 0.6402 - val_accuracy: 0.6417 - lr: 1.0000e-06 - 577ms/epoch - 5ms/step\n",
            "Epoch 32/200\n",
            "121/121 - 1s - loss: 0.5795 - accuracy: 0.6996 - val_loss: 0.6392 - val_accuracy: 0.6458 - lr: 1.0000e-06 - 599ms/epoch - 5ms/step\n",
            "Epoch 33/200\n",
            "121/121 - 1s - loss: 0.5803 - accuracy: 0.6986 - val_loss: 0.6396 - val_accuracy: 0.6446 - lr: 1.0000e-06 - 603ms/epoch - 5ms/step\n",
            "Epoch 34/200\n",
            "121/121 - 1s - loss: 0.5797 - accuracy: 0.6977 - val_loss: 0.6398 - val_accuracy: 0.6446 - lr: 1.0000e-06 - 584ms/epoch - 5ms/step\n",
            "Epoch 35/200\n",
            "121/121 - 1s - loss: 0.5791 - accuracy: 0.6971 - val_loss: 0.6395 - val_accuracy: 0.6446 - lr: 1.0000e-06 - 581ms/epoch - 5ms/step\n",
            "Epoch 36/200\n",
            "121/121 - 1s - loss: 0.5799 - accuracy: 0.6975 - val_loss: 0.6394 - val_accuracy: 0.6452 - lr: 1.0000e-06 - 582ms/epoch - 5ms/step\n",
            "Epoch 37/200\n",
            "121/121 - 1s - loss: 0.5794 - accuracy: 0.6985 - val_loss: 0.6396 - val_accuracy: 0.6446 - lr: 1.0000e-06 - 583ms/epoch - 5ms/step\n",
            "Epoch 38/200\n",
            "121/121 - 1s - loss: 0.5799 - accuracy: 0.6974 - val_loss: 0.6393 - val_accuracy: 0.6452 - lr: 1.0000e-06 - 581ms/epoch - 5ms/step\n",
            "Epoch 39/200\n",
            "121/121 - 1s - loss: 0.5792 - accuracy: 0.6987 - val_loss: 0.6394 - val_accuracy: 0.6458 - lr: 1.0000e-06 - 580ms/epoch - 5ms/step\n",
            "Epoch 40/200\n",
            "121/121 - 1s - loss: 0.5798 - accuracy: 0.6993 - val_loss: 0.6395 - val_accuracy: 0.6446 - lr: 1.0000e-06 - 580ms/epoch - 5ms/step\n",
            "Epoch 41/200\n",
            "121/121 - 1s - loss: 0.5806 - accuracy: 0.6985 - val_loss: 0.6392 - val_accuracy: 0.6464 - lr: 1.0000e-07 - 718ms/epoch - 6ms/step\n",
            "Epoch 42/200\n",
            "121/121 - 1s - loss: 0.5798 - accuracy: 0.7001 - val_loss: 0.6388 - val_accuracy: 0.6470 - lr: 1.0000e-07 - 590ms/epoch - 5ms/step\n",
            "Epoch 43/200\n",
            "121/121 - 1s - loss: 0.5794 - accuracy: 0.6994 - val_loss: 0.6387 - val_accuracy: 0.6470 - lr: 1.0000e-07 - 577ms/epoch - 5ms/step\n",
            "Epoch 44/200\n",
            "121/121 - 1s - loss: 0.5797 - accuracy: 0.6987 - val_loss: 0.6396 - val_accuracy: 0.6452 - lr: 1.0000e-07 - 583ms/epoch - 5ms/step\n",
            "Epoch 45/200\n",
            "121/121 - 1s - loss: 0.5800 - accuracy: 0.6984 - val_loss: 0.6389 - val_accuracy: 0.6470 - lr: 1.0000e-07 - 575ms/epoch - 5ms/step\n",
            "Epoch 46/200\n",
            "121/121 - 1s - loss: 0.5789 - accuracy: 0.6974 - val_loss: 0.6389 - val_accuracy: 0.6470 - lr: 1.0000e-07 - 587ms/epoch - 5ms/step\n",
            "Epoch 47/200\n",
            "121/121 - 1s - loss: 0.5796 - accuracy: 0.6972 - val_loss: 0.6391 - val_accuracy: 0.6464 - lr: 1.0000e-07 - 586ms/epoch - 5ms/step\n",
            "Epoch 48/200\n",
            "121/121 - 1s - loss: 0.5790 - accuracy: 0.6988 - val_loss: 0.6390 - val_accuracy: 0.6470 - lr: 1.0000e-07 - 575ms/epoch - 5ms/step\n",
            "Epoch 49/200\n",
            "121/121 - 1s - loss: 0.5800 - accuracy: 0.6996 - val_loss: 0.6389 - val_accuracy: 0.6464 - lr: 1.0000e-07 - 692ms/epoch - 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG 16"
      ],
      "metadata": {
        "id": "V-XDmW0porII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_16_model=Sequential()\n",
        "\n",
        "vgg_16_model.add(Conv1D(filters=64, kernel_size=3, padding='same',  input_shape=(1000,1)))\n",
        "vgg_16_model.add(BatchNormalization())\n",
        "vgg_16_model.add(Activation('relu'))\n",
        "vgg_16_model.add(Conv1D(filters=64, kernel_size=3, padding='same'))\n",
        "vgg_16_model.add(BatchNormalization())\n",
        "vgg_16_model.add(Activation('relu'))\n",
        "vgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n",
        "\n",
        "vgg_16_model.add(Conv1D(filters=128, kernel_size=3, padding='same'))\n",
        "vgg_16_model.add(BatchNormalization())\n",
        "vgg_16_model.add(Activation('relu'))\n",
        "vgg_16_model.add(Conv1D(filters=128, kernel_size=3, padding='same'))\n",
        "vgg_16_model.add(BatchNormalization())\n",
        "vgg_16_model.add(Activation('relu'))\n",
        "vgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n",
        "\n",
        "vgg_16_model.add(Conv1D(filters=256, kernel_size=3, padding='same'))\n",
        "vgg_16_model.add(BatchNormalization())\n",
        "vgg_16_model.add(Activation('relu'))\n",
        "vgg_16_model.add(Conv1D(filters=256, kernel_size=3, padding='same'))\n",
        "vgg_16_model.add(BatchNormalization())\n",
        "vgg_16_model.add(Activation('relu'))\n",
        "vgg_16_model.add(Conv1D(filters=256, kernel_size=3, padding='same'))\n",
        "vgg_16_model.add(BatchNormalization())\n",
        "vgg_16_model.add(Activation('relu'))\n",
        "vgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n",
        "\n",
        "vgg_16_model.add(Conv1D(filters=512, kernel_size=3, padding='same'))\n",
        "vgg_16_model.add(BatchNormalization())\n",
        "vgg_16_model.add(Activation('relu'))\n",
        "vgg_16_model.add(Conv1D(filters=512, kernel_size=3, padding='same'))\n",
        "vgg_16_model.add(BatchNormalization())\n",
        "vgg_16_model.add(Activation('relu'))\n",
        "vgg_16_model.add(Conv1D(filters=512, kernel_size=3, padding='same'))\n",
        "vgg_16_model.add(BatchNormalization())\n",
        "vgg_16_model.add(Activation('relu'))\n",
        "vgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n",
        "\n",
        "vgg_16_model.add(Conv1D(filters=512, kernel_size=3, padding='same'))\n",
        "vgg_16_model.add(BatchNormalization())\n",
        "vgg_16_model.add(Activation('relu'))\n",
        "vgg_16_model.add(Conv1D(filters=512, kernel_size=1, padding='same'))\n",
        "vgg_16_model.add(BatchNormalization())\n",
        "vgg_16_model.add(Activation('relu'))\n",
        "vgg_16_model.add(Conv1D(filters=512, kernel_size=1, padding='same'))\n",
        "vgg_16_model.add(BatchNormalization())\n",
        "vgg_16_model.add(Activation('relu'))\n",
        "vgg_16_model.add(MaxPool1D(pool_size=2, strides=2, padding='same'))\n",
        "\n",
        "vgg_16_model.add(GlobalAveragePooling1D())\n",
        "vgg_16_model.add(Dense(256, activation='relu'))\n",
        "vgg_16_model.add(Dropout(0.4))\n",
        "vgg_16_model.add(Dense(128, activation='relu'))\n",
        "vgg_16_model.add(Dropout(0.4))\n",
        "vgg_16_model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "YkR2QN4-otGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import math\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    # floor 내림  ex) math.floor(3.5) ->3\n",
        "    # 0.1^(epoch 10의 배수마다 내림)\n",
        "    # 10의 배수마다 0.1에 제곱한 값을 0.001에 곱해 learning rate를 낮춤\n",
        "    lr = 0.001 * math.pow(0.1, math.floor(epoch / 10))\n",
        "    return lr\n",
        "\n",
        "print('예시')\n",
        "print(lr_schedule(9), lr_schedule(10), lr_schedule(19), lr_schedule(20))\n",
        "\n",
        "# lr_scheduler에 집어넣기\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913ae51f-cd02-4127-8739-f76aba6f3304",
        "id": "paP1gCUnotOj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예시\n",
            "0.001 0.0001 0.0001 1.0000000000000003e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad\n",
        "\n",
        "# 모델 컴파일\n",
        "vgg_16_model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mH1qaA0qotOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "filepath = \"best_model_vgg_16_model.h5\"\n",
        "# early_stopping 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
        "# 체크포인트 설정\n",
        "model_checkpoint = ModelCheckpoint(filepath, save_best_only=True, save_weights_only=False, monitor='val_loss', mode='min')"
      ],
      "metadata": {
        "id": "xh9Il2NSotOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu 사용\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "\n",
        "with tf.device(device_name):\n",
        "    history = vgg_16_model.fit(x=X_train,\n",
        "                          y=y_train,           \n",
        "                          validation_split= 0.1,\n",
        "                          batch_size=128,         \n",
        "                          epochs=200,            \n",
        "                          verbose=2,\n",
        "                        callbacks=[lr_scheduler, early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp2cTllVotOm",
        "outputId": "a52d0192-8391-41e8-8a6e-22dad458edb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n",
            "Epoch 1/200\n",
            "121/121 - 30s - loss: 0.6350 - accuracy: 0.6625 - val_loss: 1.1133 - val_accuracy: 0.5123 - lr: 0.0010 - 30s/epoch - 248ms/step\n",
            "Epoch 2/200\n",
            "121/121 - 27s - loss: 0.5931 - accuracy: 0.6966 - val_loss: 0.7591 - val_accuracy: 0.5304 - lr: 0.0010 - 27s/epoch - 222ms/step\n",
            "Epoch 3/200\n",
            "121/121 - 25s - loss: 0.5768 - accuracy: 0.7018 - val_loss: 0.8544 - val_accuracy: 0.5580 - lr: 0.0010 - 25s/epoch - 206ms/step\n",
            "Epoch 4/200\n",
            "121/121 - 25s - loss: 0.5657 - accuracy: 0.7080 - val_loss: 0.7764 - val_accuracy: 0.6311 - lr: 0.0010 - 25s/epoch - 208ms/step\n",
            "Epoch 5/200\n",
            "121/121 - 27s - loss: 0.5523 - accuracy: 0.7190 - val_loss: 0.6614 - val_accuracy: 0.6639 - lr: 0.0010 - 27s/epoch - 220ms/step\n",
            "Epoch 6/200\n",
            "121/121 - 26s - loss: 0.5548 - accuracy: 0.7190 - val_loss: 0.5941 - val_accuracy: 0.6733 - lr: 0.0010 - 26s/epoch - 217ms/step\n",
            "Epoch 7/200\n",
            "121/121 - 25s - loss: 0.5444 - accuracy: 0.7277 - val_loss: 0.6273 - val_accuracy: 0.6639 - lr: 0.0010 - 25s/epoch - 208ms/step\n",
            "Epoch 8/200\n",
            "121/121 - 25s - loss: 0.5399 - accuracy: 0.7290 - val_loss: 0.6545 - val_accuracy: 0.6938 - lr: 0.0010 - 25s/epoch - 209ms/step\n",
            "Epoch 9/200\n",
            "121/121 - 27s - loss: 0.5371 - accuracy: 0.7343 - val_loss: 0.5916 - val_accuracy: 0.6885 - lr: 0.0010 - 27s/epoch - 223ms/step\n",
            "Epoch 10/200\n",
            "121/121 - 26s - loss: 0.5314 - accuracy: 0.7385 - val_loss: 0.5853 - val_accuracy: 0.6739 - lr: 0.0010 - 26s/epoch - 219ms/step\n",
            "Epoch 11/200\n",
            "121/121 - 26s - loss: 0.5155 - accuracy: 0.7461 - val_loss: 0.5767 - val_accuracy: 0.7020 - lr: 1.0000e-04 - 26s/epoch - 218ms/step\n",
            "Epoch 12/200\n",
            "121/121 - 25s - loss: 0.5074 - accuracy: 0.7543 - val_loss: 0.5906 - val_accuracy: 0.6967 - lr: 1.0000e-04 - 25s/epoch - 208ms/step\n",
            "Epoch 13/200\n",
            "121/121 - 26s - loss: 0.5007 - accuracy: 0.7578 - val_loss: 0.5736 - val_accuracy: 0.7037 - lr: 1.0000e-04 - 26s/epoch - 218ms/step\n",
            "Epoch 14/200\n",
            "121/121 - 25s - loss: 0.4963 - accuracy: 0.7597 - val_loss: 0.5838 - val_accuracy: 0.7078 - lr: 1.0000e-04 - 25s/epoch - 208ms/step\n",
            "Epoch 15/200\n",
            "121/121 - 25s - loss: 0.4927 - accuracy: 0.7657 - val_loss: 0.5854 - val_accuracy: 0.7049 - lr: 1.0000e-04 - 25s/epoch - 208ms/step\n",
            "Epoch 16/200\n",
            "121/121 - 25s - loss: 0.4901 - accuracy: 0.7654 - val_loss: 0.5801 - val_accuracy: 0.7160 - lr: 1.0000e-04 - 25s/epoch - 208ms/step\n",
            "Epoch 17/200\n",
            "121/121 - 26s - loss: 0.4874 - accuracy: 0.7662 - val_loss: 0.5677 - val_accuracy: 0.7219 - lr: 1.0000e-04 - 26s/epoch - 218ms/step\n",
            "Epoch 18/200\n",
            "121/121 - 25s - loss: 0.4841 - accuracy: 0.7698 - val_loss: 0.5818 - val_accuracy: 0.7026 - lr: 1.0000e-04 - 25s/epoch - 208ms/step\n",
            "Epoch 19/200\n",
            "121/121 - 25s - loss: 0.4806 - accuracy: 0.7748 - val_loss: 0.5906 - val_accuracy: 0.6944 - lr: 1.0000e-04 - 25s/epoch - 208ms/step\n",
            "Epoch 20/200\n",
            "121/121 - 26s - loss: 0.4784 - accuracy: 0.7724 - val_loss: 0.5640 - val_accuracy: 0.7172 - lr: 1.0000e-04 - 26s/epoch - 218ms/step\n",
            "Epoch 21/200\n",
            "121/121 - 25s - loss: 0.4650 - accuracy: 0.7852 - val_loss: 0.5655 - val_accuracy: 0.7248 - lr: 1.0000e-05 - 25s/epoch - 208ms/step\n",
            "Epoch 22/200\n",
            "121/121 - 25s - loss: 0.4633 - accuracy: 0.7853 - val_loss: 0.5661 - val_accuracy: 0.7278 - lr: 1.0000e-05 - 25s/epoch - 208ms/step\n",
            "Epoch 23/200\n",
            "121/121 - 25s - loss: 0.4630 - accuracy: 0.7875 - val_loss: 0.5654 - val_accuracy: 0.7260 - lr: 1.0000e-05 - 25s/epoch - 208ms/step\n",
            "Epoch 24/200\n",
            "121/121 - 25s - loss: 0.4617 - accuracy: 0.7864 - val_loss: 0.5642 - val_accuracy: 0.7254 - lr: 1.0000e-05 - 25s/epoch - 208ms/step\n",
            "Epoch 25/200\n",
            "121/121 - 25s - loss: 0.4612 - accuracy: 0.7869 - val_loss: 0.5642 - val_accuracy: 0.7237 - lr: 1.0000e-05 - 25s/epoch - 208ms/step\n",
            "Epoch 26/200\n",
            "121/121 - 26s - loss: 0.4587 - accuracy: 0.7895 - val_loss: 0.5630 - val_accuracy: 0.7254 - lr: 1.0000e-05 - 26s/epoch - 218ms/step\n",
            "Epoch 27/200\n",
            "121/121 - 25s - loss: 0.4562 - accuracy: 0.7883 - val_loss: 0.5652 - val_accuracy: 0.7237 - lr: 1.0000e-05 - 25s/epoch - 208ms/step\n",
            "Epoch 28/200\n",
            "121/121 - 25s - loss: 0.4557 - accuracy: 0.7881 - val_loss: 0.5641 - val_accuracy: 0.7242 - lr: 1.0000e-05 - 25s/epoch - 209ms/step\n",
            "Epoch 29/200\n",
            "121/121 - 26s - loss: 0.4545 - accuracy: 0.7888 - val_loss: 0.5616 - val_accuracy: 0.7242 - lr: 1.0000e-05 - 26s/epoch - 218ms/step\n",
            "Epoch 30/200\n",
            "121/121 - 25s - loss: 0.4565 - accuracy: 0.7899 - val_loss: 0.5658 - val_accuracy: 0.7213 - lr: 1.0000e-05 - 25s/epoch - 208ms/step\n",
            "Epoch 31/200\n",
            "121/121 - 25s - loss: 0.4526 - accuracy: 0.7932 - val_loss: 0.5638 - val_accuracy: 0.7225 - lr: 1.0000e-06 - 25s/epoch - 208ms/step\n",
            "Epoch 32/200\n",
            "121/121 - 25s - loss: 0.4527 - accuracy: 0.7903 - val_loss: 0.5638 - val_accuracy: 0.7225 - lr: 1.0000e-06 - 25s/epoch - 208ms/step\n",
            "Epoch 33/200\n",
            "121/121 - 25s - loss: 0.4525 - accuracy: 0.7905 - val_loss: 0.5641 - val_accuracy: 0.7248 - lr: 1.0000e-06 - 25s/epoch - 208ms/step\n",
            "Epoch 34/200\n",
            "121/121 - 25s - loss: 0.4530 - accuracy: 0.7888 - val_loss: 0.5638 - val_accuracy: 0.7242 - lr: 1.0000e-06 - 25s/epoch - 208ms/step\n",
            "Epoch 35/200\n",
            "121/121 - 25s - loss: 0.4529 - accuracy: 0.7910 - val_loss: 0.5629 - val_accuracy: 0.7254 - lr: 1.0000e-06 - 25s/epoch - 208ms/step\n",
            "Epoch 36/200\n",
            "121/121 - 25s - loss: 0.4521 - accuracy: 0.7923 - val_loss: 0.5637 - val_accuracy: 0.7260 - lr: 1.0000e-06 - 25s/epoch - 208ms/step\n",
            "Epoch 37/200\n",
            "121/121 - 25s - loss: 0.4523 - accuracy: 0.7914 - val_loss: 0.5637 - val_accuracy: 0.7266 - lr: 1.0000e-06 - 25s/epoch - 208ms/step\n",
            "Epoch 38/200\n",
            "121/121 - 25s - loss: 0.4511 - accuracy: 0.7938 - val_loss: 0.5642 - val_accuracy: 0.7260 - lr: 1.0000e-06 - 25s/epoch - 208ms/step\n",
            "Epoch 39/200\n",
            "121/121 - 25s - loss: 0.4534 - accuracy: 0.7932 - val_loss: 0.5638 - val_accuracy: 0.7266 - lr: 1.0000e-06 - 25s/epoch - 208ms/step\n",
            "Epoch 40/200\n",
            "121/121 - 25s - loss: 0.4528 - accuracy: 0.7931 - val_loss: 0.5638 - val_accuracy: 0.7283 - lr: 1.0000e-06 - 25s/epoch - 208ms/step\n",
            "Epoch 41/200\n",
            "121/121 - 25s - loss: 0.4513 - accuracy: 0.7915 - val_loss: 0.5641 - val_accuracy: 0.7272 - lr: 1.0000e-07 - 25s/epoch - 208ms/step\n",
            "Epoch 42/200\n",
            "121/121 - 25s - loss: 0.4505 - accuracy: 0.7912 - val_loss: 0.5646 - val_accuracy: 0.7260 - lr: 1.0000e-07 - 25s/epoch - 208ms/step\n",
            "Epoch 43/200\n",
            "121/121 - 25s - loss: 0.4529 - accuracy: 0.7902 - val_loss: 0.5635 - val_accuracy: 0.7278 - lr: 1.0000e-07 - 25s/epoch - 208ms/step\n",
            "Epoch 44/200\n",
            "121/121 - 25s - loss: 0.4513 - accuracy: 0.7897 - val_loss: 0.5640 - val_accuracy: 0.7278 - lr: 1.0000e-07 - 25s/epoch - 208ms/step\n",
            "Epoch 45/200\n",
            "121/121 - 25s - loss: 0.4523 - accuracy: 0.7917 - val_loss: 0.5642 - val_accuracy: 0.7278 - lr: 1.0000e-07 - 25s/epoch - 208ms/step\n",
            "Epoch 46/200\n",
            "121/121 - 25s - loss: 0.4507 - accuracy: 0.7914 - val_loss: 0.5642 - val_accuracy: 0.7266 - lr: 1.0000e-07 - 25s/epoch - 208ms/step\n",
            "Epoch 47/200\n",
            "121/121 - 25s - loss: 0.4528 - accuracy: 0.7918 - val_loss: 0.5639 - val_accuracy: 0.7272 - lr: 1.0000e-07 - 25s/epoch - 208ms/step\n",
            "Epoch 48/200\n",
            "121/121 - 25s - loss: 0.4505 - accuracy: 0.7934 - val_loss: 0.5640 - val_accuracy: 0.7272 - lr: 1.0000e-07 - 25s/epoch - 208ms/step\n",
            "Epoch 49/200\n",
            "121/121 - 25s - loss: 0.4508 - accuracy: 0.7908 - val_loss: 0.5639 - val_accuracy: 0.7272 - lr: 1.0000e-07 - 25s/epoch - 208ms/step\n",
            "Epoch 50/200\n",
            "121/121 - 25s - loss: 0.4515 - accuracy: 0.7918 - val_loss: 0.5642 - val_accuracy: 0.7272 - lr: 1.0000e-07 - 25s/epoch - 208ms/step\n",
            "Epoch 51/200\n",
            "121/121 - 25s - loss: 0.4511 - accuracy: 0.7929 - val_loss: 0.5634 - val_accuracy: 0.7266 - lr: 1.0000e-08 - 25s/epoch - 208ms/step\n",
            "Epoch 52/200\n",
            "121/121 - 25s - loss: 0.4494 - accuracy: 0.7940 - val_loss: 0.5638 - val_accuracy: 0.7266 - lr: 1.0000e-08 - 25s/epoch - 208ms/step\n",
            "Epoch 53/200\n",
            "121/121 - 25s - loss: 0.4525 - accuracy: 0.7908 - val_loss: 0.5638 - val_accuracy: 0.7266 - lr: 1.0000e-08 - 25s/epoch - 208ms/step\n",
            "Epoch 54/200\n",
            "121/121 - 25s - loss: 0.4504 - accuracy: 0.7927 - val_loss: 0.5637 - val_accuracy: 0.7272 - lr: 1.0000e-08 - 25s/epoch - 208ms/step\n",
            "Epoch 55/200\n",
            "121/121 - 25s - loss: 0.4491 - accuracy: 0.7916 - val_loss: 0.5638 - val_accuracy: 0.7272 - lr: 1.0000e-08 - 25s/epoch - 208ms/step\n",
            "Epoch 56/200\n",
            "121/121 - 25s - loss: 0.4520 - accuracy: 0.7930 - val_loss: 0.5633 - val_accuracy: 0.7278 - lr: 1.0000e-08 - 25s/epoch - 208ms/step\n",
            "Epoch 57/200\n",
            "121/121 - 25s - loss: 0.4489 - accuracy: 0.7910 - val_loss: 0.5643 - val_accuracy: 0.7266 - lr: 1.0000e-08 - 25s/epoch - 208ms/step\n",
            "Epoch 58/200\n",
            "121/121 - 25s - loss: 0.4504 - accuracy: 0.7919 - val_loss: 0.5642 - val_accuracy: 0.7272 - lr: 1.0000e-08 - 25s/epoch - 208ms/step\n",
            "Epoch 59/200\n",
            "121/121 - 25s - loss: 0.4526 - accuracy: 0.7898 - val_loss: 0.5637 - val_accuracy: 0.7283 - lr: 1.0000e-08 - 25s/epoch - 208ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inception"
      ],
      "metadata": {
        "id": "ZVxKatGkpgf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import concatenate"
      ],
      "metadata": {
        "id": "scmpoO5JrEg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inception_block(prev_layer):\n",
        "    \n",
        "    conv1=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(prev_layer)\n",
        "    conv1=BatchNormalization()(conv1)\n",
        "    conv1=Activation('relu')(conv1)\n",
        "    \n",
        "    conv3=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(prev_layer)\n",
        "    conv3=BatchNormalization()(conv3)\n",
        "    conv3=Activation('relu')(conv3)\n",
        "    conv3=Conv1D(filters = 64, kernel_size = 3, padding = 'same')(conv3)\n",
        "    conv3=BatchNormalization()(conv3)\n",
        "    conv3=Activation('relu')(conv3)\n",
        "    \n",
        "    conv5=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(prev_layer)\n",
        "    conv5=BatchNormalization()(conv5)\n",
        "    conv5=Activation('relu')(conv5)\n",
        "    conv5=Conv1D(filters = 64, kernel_size = 5, padding = 'same')(conv5)\n",
        "    conv5=BatchNormalization()(conv5)\n",
        "    conv5=Activation('relu')(conv5)\n",
        "    \n",
        "    pool= MaxPool1D(pool_size=3, strides=1, padding='same')(prev_layer)\n",
        "    convmax=Conv1D(filters = 64, kernel_size = 1, padding = 'same')(pool)\n",
        "    convmax=BatchNormalization()(convmax)\n",
        "    convmax=Activation('relu')(convmax)\n",
        "    \n",
        "    layer_out = concatenate([conv1, conv3, conv5, convmax], axis=1)\n",
        "    \n",
        "    return layer_out\n",
        "\n",
        "def inception_model(input_shape):\n",
        "    X_input=Input(input_shape)\n",
        "    \n",
        "    X = ZeroPadding1D(3)(X_input)\n",
        "    \n",
        "    X = Conv1D(filters = 64, kernel_size = 7, padding = 'same')(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPool1D(pool_size=3, strides=2, padding='same')(X)\n",
        "    \n",
        "    X = Conv1D(filters = 64, kernel_size = 1, padding = 'same')(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    X = inception_block(X)\n",
        "    X = inception_block(X)\n",
        "    \n",
        "    X = MaxPool1D(pool_size=7, strides=2, padding='same')(X)\n",
        "    \n",
        "    X = GlobalAveragePooling1D()(X)\n",
        "    X = Dense(1,activation='sigmoid')(X)\n",
        "    \n",
        "    model = Model(inputs = X_input, outputs = X, name='Inception')\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "sGKi_qnIpgbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inception_model = inception_model(input_shape = (1000,1))"
      ],
      "metadata": {
        "id": "PF8pP3m8pm8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import math\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    # floor 내림  ex) math.floor(3.5) ->3\n",
        "    # 0.1^(epoch 10의 배수마다 내림)\n",
        "    # 10의 배수마다 0.1에 제곱한 값을 0.001에 곱해 learning rate를 낮춤\n",
        "    lr = 0.001 * math.pow(0.1, math.floor(epoch / 10))\n",
        "    return lr\n",
        "\n",
        "print('예시')\n",
        "print(lr_schedule(9), lr_schedule(10), lr_schedule(19), lr_schedule(20))\n",
        "\n",
        "# lr_scheduler에 집어넣기\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c877e895-ed88-499d-e89f-66740912c048",
        "id": "Zc2sihxlpnDG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예시\n",
            "0.001 0.0001 0.0001 1.0000000000000003e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad\n",
        "\n",
        "# 모델 컴파일\n",
        "inception_model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "V6FKkFAspnDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "filepath = \"best_model_inception_model.h5\"\n",
        "# early_stopping 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
        "# 체크포인트 설정\n",
        "model_checkpoint = ModelCheckpoint(filepath, save_best_only=True, save_weights_only=False, monitor='val_loss', mode='min')"
      ],
      "metadata": {
        "id": "J5nUg8AkpnDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu 사용\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "\n",
        "with tf.device(device_name):\n",
        "    history = inception_model.fit(x=X_train,\n",
        "                          y=y_train,           \n",
        "                          validation_split= 0.1,\n",
        "                          batch_size=128,         \n",
        "                          epochs=200,            \n",
        "                          verbose=2,\n",
        "                        callbacks=[lr_scheduler, early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5Non--ypnDI",
        "outputId": "7c284836-487e-465f-a144-2657eac2f2e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n",
            "Epoch 1/200\n",
            "121/121 - 30s - loss: 0.6062 - accuracy: 0.6749 - val_loss: 0.7489 - val_accuracy: 0.5129 - lr: 0.0010 - 30s/epoch - 245ms/step\n",
            "Epoch 2/200\n",
            "121/121 - 25s - loss: 0.5822 - accuracy: 0.6998 - val_loss: 0.8223 - val_accuracy: 0.5123 - lr: 0.0010 - 25s/epoch - 207ms/step\n",
            "Epoch 3/200\n",
            "121/121 - 25s - loss: 0.5717 - accuracy: 0.7064 - val_loss: 0.9109 - val_accuracy: 0.5187 - lr: 0.0010 - 25s/epoch - 206ms/step\n",
            "Epoch 4/200\n",
            "121/121 - 25s - loss: 0.5647 - accuracy: 0.7136 - val_loss: 0.7951 - val_accuracy: 0.5597 - lr: 0.0010 - 25s/epoch - 207ms/step\n",
            "Epoch 5/200\n",
            "121/121 - 25s - loss: 0.5595 - accuracy: 0.7150 - val_loss: 0.7642 - val_accuracy: 0.5826 - lr: 0.0010 - 25s/epoch - 207ms/step\n",
            "Epoch 6/200\n",
            "121/121 - 26s - loss: 0.5565 - accuracy: 0.7171 - val_loss: 0.6671 - val_accuracy: 0.6019 - lr: 0.0010 - 26s/epoch - 212ms/step\n",
            "Epoch 7/200\n",
            "121/121 - 26s - loss: 0.5533 - accuracy: 0.7170 - val_loss: 0.6553 - val_accuracy: 0.6311 - lr: 0.0010 - 26s/epoch - 213ms/step\n",
            "Epoch 8/200\n",
            "121/121 - 25s - loss: 0.5490 - accuracy: 0.7238 - val_loss: 0.7391 - val_accuracy: 0.5796 - lr: 0.0010 - 25s/epoch - 206ms/step\n",
            "Epoch 9/200\n",
            "121/121 - 26s - loss: 0.5435 - accuracy: 0.7285 - val_loss: 0.6272 - val_accuracy: 0.6540 - lr: 0.0010 - 26s/epoch - 212ms/step\n",
            "Epoch 10/200\n",
            "121/121 - 25s - loss: 0.5442 - accuracy: 0.7270 - val_loss: 0.7055 - val_accuracy: 0.6183 - lr: 0.0010 - 25s/epoch - 206ms/step\n",
            "Epoch 11/200\n",
            "121/121 - 26s - loss: 0.5309 - accuracy: 0.7352 - val_loss: 0.6225 - val_accuracy: 0.6651 - lr: 1.0000e-04 - 26s/epoch - 212ms/step\n",
            "Epoch 12/200\n",
            "121/121 - 26s - loss: 0.5261 - accuracy: 0.7429 - val_loss: 0.5991 - val_accuracy: 0.6727 - lr: 1.0000e-04 - 26s/epoch - 215ms/step\n",
            "Epoch 13/200\n",
            "121/121 - 26s - loss: 0.5241 - accuracy: 0.7415 - val_loss: 0.5856 - val_accuracy: 0.6862 - lr: 1.0000e-04 - 26s/epoch - 213ms/step\n",
            "Epoch 14/200\n",
            "121/121 - 25s - loss: 0.5242 - accuracy: 0.7404 - val_loss: 0.6277 - val_accuracy: 0.6516 - lr: 1.0000e-04 - 25s/epoch - 207ms/step\n",
            "Epoch 15/200\n",
            "121/121 - 25s - loss: 0.5227 - accuracy: 0.7438 - val_loss: 0.6066 - val_accuracy: 0.6721 - lr: 1.0000e-04 - 25s/epoch - 207ms/step\n",
            "Epoch 16/200\n",
            "121/121 - 26s - loss: 0.5213 - accuracy: 0.7433 - val_loss: 0.5806 - val_accuracy: 0.6961 - lr: 1.0000e-04 - 26s/epoch - 213ms/step\n",
            "Epoch 17/200\n",
            "121/121 - 26s - loss: 0.5212 - accuracy: 0.7442 - val_loss: 0.5798 - val_accuracy: 0.6932 - lr: 1.0000e-04 - 26s/epoch - 213ms/step\n",
            "Epoch 18/200\n",
            "121/121 - 25s - loss: 0.5193 - accuracy: 0.7476 - val_loss: 0.6026 - val_accuracy: 0.6745 - lr: 1.0000e-04 - 25s/epoch - 207ms/step\n",
            "Epoch 19/200\n",
            "121/121 - 25s - loss: 0.5182 - accuracy: 0.7478 - val_loss: 0.5970 - val_accuracy: 0.6827 - lr: 1.0000e-04 - 25s/epoch - 207ms/step\n",
            "Epoch 20/200\n",
            "121/121 - 26s - loss: 0.5189 - accuracy: 0.7472 - val_loss: 0.6067 - val_accuracy: 0.6674 - lr: 1.0000e-04 - 26s/epoch - 211ms/step\n",
            "Epoch 21/200\n",
            "121/121 - 25s - loss: 0.5155 - accuracy: 0.7508 - val_loss: 0.5852 - val_accuracy: 0.6950 - lr: 1.0000e-05 - 25s/epoch - 207ms/step\n",
            "Epoch 22/200\n",
            "121/121 - 25s - loss: 0.5154 - accuracy: 0.7491 - val_loss: 0.5837 - val_accuracy: 0.6950 - lr: 1.0000e-05 - 25s/epoch - 207ms/step\n",
            "Epoch 23/200\n",
            "121/121 - 25s - loss: 0.5149 - accuracy: 0.7495 - val_loss: 0.5839 - val_accuracy: 0.6944 - lr: 1.0000e-05 - 25s/epoch - 207ms/step\n",
            "Epoch 24/200\n",
            "121/121 - 25s - loss: 0.5149 - accuracy: 0.7515 - val_loss: 0.5821 - val_accuracy: 0.6920 - lr: 1.0000e-05 - 25s/epoch - 206ms/step\n",
            "Epoch 25/200\n",
            "121/121 - 25s - loss: 0.5155 - accuracy: 0.7489 - val_loss: 0.5813 - val_accuracy: 0.6938 - lr: 1.0000e-05 - 25s/epoch - 206ms/step\n",
            "Epoch 26/200\n",
            "121/121 - 25s - loss: 0.5144 - accuracy: 0.7500 - val_loss: 0.5818 - val_accuracy: 0.6944 - lr: 1.0000e-05 - 25s/epoch - 206ms/step\n",
            "Epoch 27/200\n",
            "121/121 - 25s - loss: 0.5142 - accuracy: 0.7500 - val_loss: 0.5802 - val_accuracy: 0.6944 - lr: 1.0000e-05 - 25s/epoch - 206ms/step\n",
            "Epoch 28/200\n",
            "121/121 - 25s - loss: 0.5146 - accuracy: 0.7508 - val_loss: 0.5876 - val_accuracy: 0.6909 - lr: 1.0000e-05 - 25s/epoch - 206ms/step\n",
            "Epoch 29/200\n",
            "121/121 - 25s - loss: 0.5141 - accuracy: 0.7491 - val_loss: 0.5843 - val_accuracy: 0.6903 - lr: 1.0000e-05 - 25s/epoch - 206ms/step\n",
            "Epoch 30/200\n",
            "121/121 - 25s - loss: 0.5143 - accuracy: 0.7495 - val_loss: 0.5822 - val_accuracy: 0.6926 - lr: 1.0000e-05 - 25s/epoch - 207ms/step\n",
            "Epoch 31/200\n",
            "121/121 - 25s - loss: 0.5141 - accuracy: 0.7500 - val_loss: 0.5818 - val_accuracy: 0.6920 - lr: 1.0000e-06 - 25s/epoch - 207ms/step\n",
            "Epoch 32/200\n",
            "121/121 - 25s - loss: 0.5138 - accuracy: 0.7504 - val_loss: 0.5823 - val_accuracy: 0.6915 - lr: 1.0000e-06 - 25s/epoch - 207ms/step\n",
            "Epoch 33/200\n",
            "121/121 - 25s - loss: 0.5133 - accuracy: 0.7504 - val_loss: 0.5821 - val_accuracy: 0.6909 - lr: 1.0000e-06 - 25s/epoch - 207ms/step\n",
            "Epoch 34/200\n",
            "121/121 - 25s - loss: 0.5139 - accuracy: 0.7487 - val_loss: 0.5823 - val_accuracy: 0.6926 - lr: 1.0000e-06 - 25s/epoch - 207ms/step\n",
            "Epoch 35/200\n",
            "121/121 - 25s - loss: 0.5141 - accuracy: 0.7510 - val_loss: 0.5816 - val_accuracy: 0.6926 - lr: 1.0000e-06 - 25s/epoch - 206ms/step\n",
            "Epoch 36/200\n",
            "121/121 - 25s - loss: 0.5136 - accuracy: 0.7502 - val_loss: 0.5827 - val_accuracy: 0.6920 - lr: 1.0000e-06 - 25s/epoch - 206ms/step\n",
            "Epoch 37/200\n",
            "121/121 - 25s - loss: 0.5136 - accuracy: 0.7502 - val_loss: 0.5825 - val_accuracy: 0.6920 - lr: 1.0000e-06 - 25s/epoch - 206ms/step\n",
            "Epoch 38/200\n",
            "121/121 - 25s - loss: 0.5142 - accuracy: 0.7491 - val_loss: 0.5830 - val_accuracy: 0.6920 - lr: 1.0000e-06 - 25s/epoch - 206ms/step\n",
            "Epoch 39/200\n",
            "121/121 - 25s - loss: 0.5147 - accuracy: 0.7502 - val_loss: 0.5824 - val_accuracy: 0.6920 - lr: 1.0000e-06 - 25s/epoch - 207ms/step\n",
            "Epoch 40/200\n",
            "121/121 - 25s - loss: 0.5131 - accuracy: 0.7509 - val_loss: 0.5827 - val_accuracy: 0.6909 - lr: 1.0000e-06 - 25s/epoch - 207ms/step\n",
            "Epoch 41/200\n",
            "121/121 - 25s - loss: 0.5133 - accuracy: 0.7520 - val_loss: 0.5828 - val_accuracy: 0.6920 - lr: 1.0000e-07 - 25s/epoch - 207ms/step\n",
            "Epoch 42/200\n",
            "121/121 - 25s - loss: 0.5136 - accuracy: 0.7521 - val_loss: 0.5829 - val_accuracy: 0.6909 - lr: 1.0000e-07 - 25s/epoch - 207ms/step\n",
            "Epoch 43/200\n",
            "121/121 - 25s - loss: 0.5129 - accuracy: 0.7519 - val_loss: 0.5832 - val_accuracy: 0.6920 - lr: 1.0000e-07 - 25s/epoch - 206ms/step\n",
            "Epoch 44/200\n",
            "121/121 - 25s - loss: 0.5148 - accuracy: 0.7478 - val_loss: 0.5828 - val_accuracy: 0.6920 - lr: 1.0000e-07 - 25s/epoch - 206ms/step\n",
            "Epoch 45/200\n",
            "121/121 - 25s - loss: 0.5149 - accuracy: 0.7479 - val_loss: 0.5830 - val_accuracy: 0.6915 - lr: 1.0000e-07 - 25s/epoch - 206ms/step\n",
            "Epoch 46/200\n",
            "121/121 - 25s - loss: 0.5133 - accuracy: 0.7512 - val_loss: 0.5824 - val_accuracy: 0.6920 - lr: 1.0000e-07 - 25s/epoch - 206ms/step\n",
            "Epoch 47/200\n",
            "121/121 - 25s - loss: 0.5137 - accuracy: 0.7490 - val_loss: 0.5823 - val_accuracy: 0.6926 - lr: 1.0000e-07 - 25s/epoch - 207ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "Q9WCPS3AouCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(64, input_shape=(1000,1), return_sequences=True))\n",
        "lstm_model.add(LSTM(64))\n",
        "lstm_model.add(Dense(32, activation = 'relu'))\n",
        "lstm_model.add(Dropout(0.3))\n",
        "lstm_model.add(Dense(1, activation = 'sigmoid'))"
      ],
      "metadata": {
        "id": "jKE7XUlEowY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import math\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    # floor 내림  ex) math.floor(3.5) ->3\n",
        "    # 0.1^(epoch 10의 배수마다 내림)\n",
        "    # 10의 배수마다 0.1에 제곱한 값을 0.001에 곱해 learning rate를 낮춤\n",
        "    lr = 0.001 * math.pow(0.1, math.floor(epoch / 10))\n",
        "    return lr\n",
        "\n",
        "print('예시')\n",
        "print(lr_schedule(9), lr_schedule(10), lr_schedule(19), lr_schedule(20))\n",
        "\n",
        "# lr_scheduler에 집어넣기\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c14225e-7299-44ba-871e-2bb4dda5e684",
        "id": "mVuiNO8Aowpt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예시\n",
            "0.001 0.0001 0.0001 1.0000000000000003e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad\n",
        "\n",
        "# 모델 컴파일\n",
        "lstm_model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "cS-IVEjUowpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "filepath = \"best_model_lstm_model.h5\"\n",
        "# early_stopping 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
        "# 체크포인트 설정\n",
        "model_checkpoint = ModelCheckpoint(filepath, save_best_only=True, save_weights_only=False, monitor='val_loss', mode='min')"
      ],
      "metadata": {
        "id": "X2myH9yBowpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu 사용\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "\n",
        "with tf.device(device_name):\n",
        "    history = lstm_model.fit(x=X_train,\n",
        "                          y=y_train,           \n",
        "                          validation_split= 0.1,\n",
        "                          batch_size=128,         \n",
        "                          epochs=200,            \n",
        "                          verbose=2,\n",
        "                        callbacks=[lr_scheduler, early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfF9iefMowpu",
        "outputId": "d3d1197c-bf97-4a47-8a88-62eeea9f6b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n",
            "Epoch 1/200\n",
            "121/121 - 13s - loss: 0.6722 - accuracy: 0.6105 - val_loss: 0.7127 - val_accuracy: 0.5129 - lr: 0.0010 - 13s/epoch - 110ms/step\n",
            "Epoch 2/200\n",
            "121/121 - 9s - loss: 0.6687 - accuracy: 0.6117 - val_loss: 0.7080 - val_accuracy: 0.5129 - lr: 0.0010 - 9s/epoch - 76ms/step\n",
            "Epoch 3/200\n",
            "121/121 - 9s - loss: 0.6684 - accuracy: 0.6119 - val_loss: 0.7114 - val_accuracy: 0.5129 - lr: 0.0010 - 9s/epoch - 76ms/step\n",
            "Epoch 4/200\n",
            "121/121 - 10s - loss: 0.6686 - accuracy: 0.6117 - val_loss: 0.7065 - val_accuracy: 0.5111 - lr: 0.0010 - 10s/epoch - 81ms/step\n",
            "Epoch 5/200\n",
            "121/121 - 9s - loss: 0.6676 - accuracy: 0.6130 - val_loss: 0.7075 - val_accuracy: 0.5129 - lr: 0.0010 - 9s/epoch - 76ms/step\n",
            "Epoch 6/200\n",
            "121/121 - 9s - loss: 0.6695 - accuracy: 0.6105 - val_loss: 0.7126 - val_accuracy: 0.5129 - lr: 0.0010 - 9s/epoch - 75ms/step\n",
            "Epoch 7/200\n",
            "121/121 - 9s - loss: 0.6697 - accuracy: 0.6119 - val_loss: 0.7120 - val_accuracy: 0.5129 - lr: 0.0010 - 9s/epoch - 76ms/step\n",
            "Epoch 8/200\n",
            "121/121 - 9s - loss: 0.6693 - accuracy: 0.6119 - val_loss: 0.7148 - val_accuracy: 0.5129 - lr: 0.0010 - 9s/epoch - 76ms/step\n",
            "Epoch 9/200\n",
            "121/121 - 9s - loss: 0.6698 - accuracy: 0.6119 - val_loss: 0.7109 - val_accuracy: 0.5129 - lr: 0.0010 - 9s/epoch - 76ms/step\n",
            "Epoch 10/200\n",
            "121/121 - 9s - loss: 0.6681 - accuracy: 0.6119 - val_loss: 0.7094 - val_accuracy: 0.5129 - lr: 0.0010 - 9s/epoch - 76ms/step\n",
            "Epoch 11/200\n",
            "121/121 - 9s - loss: 0.6685 - accuracy: 0.6119 - val_loss: 0.7101 - val_accuracy: 0.5129 - lr: 1.0000e-04 - 9s/epoch - 77ms/step\n",
            "Epoch 12/200\n",
            "121/121 - 9s - loss: 0.6693 - accuracy: 0.6119 - val_loss: 0.7095 - val_accuracy: 0.5129 - lr: 1.0000e-04 - 9s/epoch - 76ms/step\n",
            "Epoch 13/200\n",
            "121/121 - 9s - loss: 0.6690 - accuracy: 0.6119 - val_loss: 0.7100 - val_accuracy: 0.5129 - lr: 1.0000e-04 - 9s/epoch - 76ms/step\n",
            "Epoch 14/200\n",
            "121/121 - 9s - loss: 0.6689 - accuracy: 0.6119 - val_loss: 0.7097 - val_accuracy: 0.5129 - lr: 1.0000e-04 - 9s/epoch - 76ms/step\n",
            "Epoch 15/200\n",
            "121/121 - 9s - loss: 0.6690 - accuracy: 0.6119 - val_loss: 0.7101 - val_accuracy: 0.5129 - lr: 1.0000e-04 - 9s/epoch - 75ms/step\n",
            "Epoch 16/200\n",
            "121/121 - 9s - loss: 0.6683 - accuracy: 0.6119 - val_loss: 0.7106 - val_accuracy: 0.5129 - lr: 1.0000e-04 - 9s/epoch - 76ms/step\n",
            "Epoch 17/200\n",
            "121/121 - 10s - loss: 0.6688 - accuracy: 0.6119 - val_loss: 0.7101 - val_accuracy: 0.5129 - lr: 1.0000e-04 - 10s/epoch - 80ms/step\n",
            "Epoch 18/200\n",
            "121/121 - 9s - loss: 0.6680 - accuracy: 0.6119 - val_loss: 0.7108 - val_accuracy: 0.5129 - lr: 1.0000e-04 - 9s/epoch - 75ms/step\n",
            "Epoch 19/200\n",
            "121/121 - 9s - loss: 0.6684 - accuracy: 0.6119 - val_loss: 0.7110 - val_accuracy: 0.5129 - lr: 1.0000e-04 - 9s/epoch - 75ms/step\n",
            "Epoch 20/200\n",
            "121/121 - 9s - loss: 0.6690 - accuracy: 0.6119 - val_loss: 0.7101 - val_accuracy: 0.5129 - lr: 1.0000e-04 - 9s/epoch - 76ms/step\n",
            "Epoch 21/200\n",
            "121/121 - 9s - loss: 0.6688 - accuracy: 0.6119 - val_loss: 0.7101 - val_accuracy: 0.5129 - lr: 1.0000e-05 - 9s/epoch - 77ms/step\n",
            "Epoch 22/200\n",
            "121/121 - 9s - loss: 0.6687 - accuracy: 0.6119 - val_loss: 0.7101 - val_accuracy: 0.5129 - lr: 1.0000e-05 - 9s/epoch - 76ms/step\n",
            "Epoch 23/200\n",
            "121/121 - 9s - loss: 0.6688 - accuracy: 0.6119 - val_loss: 0.7101 - val_accuracy: 0.5129 - lr: 1.0000e-05 - 9s/epoch - 75ms/step\n",
            "Epoch 24/200\n",
            "121/121 - 9s - loss: 0.6685 - accuracy: 0.6119 - val_loss: 0.7102 - val_accuracy: 0.5129 - lr: 1.0000e-05 - 9s/epoch - 77ms/step\n",
            "Epoch 25/200\n",
            "121/121 - 9s - loss: 0.6690 - accuracy: 0.6119 - val_loss: 0.7101 - val_accuracy: 0.5129 - lr: 1.0000e-05 - 9s/epoch - 76ms/step\n",
            "Epoch 26/200\n",
            "121/121 - 9s - loss: 0.6690 - accuracy: 0.6119 - val_loss: 0.7101 - val_accuracy: 0.5129 - lr: 1.0000e-05 - 9s/epoch - 77ms/step\n",
            "Epoch 27/200\n",
            "121/121 - 9s - loss: 0.6692 - accuracy: 0.6119 - val_loss: 0.7100 - val_accuracy: 0.5129 - lr: 1.0000e-05 - 9s/epoch - 76ms/step\n",
            "Epoch 28/200\n",
            "121/121 - 9s - loss: 0.6690 - accuracy: 0.6119 - val_loss: 0.7100 - val_accuracy: 0.5129 - lr: 1.0000e-05 - 9s/epoch - 75ms/step\n",
            "Epoch 29/200\n",
            "121/121 - 9s - loss: 0.6689 - accuracy: 0.6119 - val_loss: 0.7099 - val_accuracy: 0.5129 - lr: 1.0000e-05 - 9s/epoch - 76ms/step\n",
            "Epoch 30/200\n",
            "121/121 - 9s - loss: 0.6686 - accuracy: 0.6119 - val_loss: 0.7099 - val_accuracy: 0.5129 - lr: 1.0000e-05 - 9s/epoch - 75ms/step\n",
            "Epoch 31/200\n",
            "121/121 - 9s - loss: 0.6686 - accuracy: 0.6119 - val_loss: 0.7099 - val_accuracy: 0.5129 - lr: 1.0000e-06 - 9s/epoch - 75ms/step\n",
            "Epoch 32/200\n",
            "121/121 - 9s - loss: 0.6690 - accuracy: 0.6119 - val_loss: 0.7099 - val_accuracy: 0.5129 - lr: 1.0000e-06 - 9s/epoch - 75ms/step\n",
            "Epoch 33/200\n",
            "121/121 - 9s - loss: 0.6686 - accuracy: 0.6119 - val_loss: 0.7099 - val_accuracy: 0.5129 - lr: 1.0000e-06 - 9s/epoch - 76ms/step\n",
            "Epoch 34/200\n",
            "121/121 - 10s - loss: 0.6687 - accuracy: 0.6119 - val_loss: 0.7099 - val_accuracy: 0.5129 - lr: 1.0000e-06 - 10s/epoch - 79ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yd6z6t3qoxHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU"
      ],
      "metadata": {
        "id": "TKySOQbZoxMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import GRU"
      ],
      "metadata": {
        "id": "NhMmTpXRox-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GRU_model = Sequential()\n",
        "GRU_model.add(GRU(64, input_shape=(1000,1), return_sequences=True))\n",
        "GRU_model.add(GRU(64))\n",
        "GRU_model.add(Dense(32, activation = 'relu'))\n",
        "GRU_model.add(Dropout(0.3))\n",
        "GRU_model.add(Dense(1, activation = 'sigmoid'))"
      ],
      "metadata": {
        "id": "g5Zqrxilqd8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import math\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    # floor 내림  ex) math.floor(3.5) ->3\n",
        "    # 0.1^(epoch 10의 배수마다 내림)\n",
        "    # 10의 배수마다 0.1에 제곱한 값을 0.001에 곱해 learning rate를 낮춤\n",
        "    lr = 0.001 * math.pow(0.1, math.floor(epoch / 10))\n",
        "    return lr\n",
        "\n",
        "print('예시')\n",
        "print(lr_schedule(9), lr_schedule(10), lr_schedule(19), lr_schedule(20))\n",
        "\n",
        "# lr_scheduler에 집어넣기\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ifSRRuPqd8x",
        "outputId": "4c6b2f31-e4ec-4d18-bc41-1fbc7cbf0f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예시\n",
            "0.001 0.0001 0.0001 1.0000000000000003e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad\n",
        "\n",
        "# 모델 컴파일\n",
        "GRU_model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "2bT_nkEJqd8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "filepath = \"best_model_GRU_model.h5\"\n",
        "# early_stopping 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
        "# 체크포인트 설정\n",
        "model_checkpoint = ModelCheckpoint(filepath, save_best_only=True, save_weights_only=False, monitor='val_loss', mode='min')"
      ],
      "metadata": {
        "id": "pBM7K5BBqd8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu 사용\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "\n",
        "with tf.device(device_name):\n",
        "    history = GRU_model.fit(x=X_train,\n",
        "                          y=y_train,           \n",
        "                          validation_split= 0.1,\n",
        "                          batch_size=128,         \n",
        "                          epochs=200,            \n",
        "                          verbose=2,\n",
        "                        callbacks=[lr_scheduler, early_stopping, model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpsmBmogqd8y",
        "outputId": "fea679cb-d7df-41ff-ab1a-4f845ee727b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n",
            "Epoch 1/200\n",
            "121/121 - 11s - loss: 0.6709 - accuracy: 0.6112 - val_loss: 0.7108 - val_accuracy: 0.5129 - lr: 0.0010 - 11s/epoch - 87ms/step\n",
            "Epoch 2/200\n",
            "121/121 - 7s - loss: 0.6693 - accuracy: 0.6116 - val_loss: 0.7058 - val_accuracy: 0.5129 - lr: 0.0010 - 7s/epoch - 61ms/step\n",
            "Epoch 3/200\n",
            "121/121 - 7s - loss: 0.6698 - accuracy: 0.6119 - val_loss: 0.7182 - val_accuracy: 0.5129 - lr: 0.0010 - 7s/epoch - 61ms/step\n",
            "Epoch 4/200\n",
            "121/121 - 7s - loss: 0.6689 - accuracy: 0.6118 - val_loss: 0.7095 - val_accuracy: 0.5129 - lr: 0.0010 - 7s/epoch - 60ms/step\n",
            "Epoch 5/200\n",
            "121/121 - 7s - loss: 0.6688 - accuracy: 0.6119 - val_loss: 0.7166 - val_accuracy: 0.5123 - lr: 0.0010 - 7s/epoch - 60ms/step\n",
            "Epoch 6/200\n",
            "121/121 - 7s - loss: 0.6687 - accuracy: 0.6119 - val_loss: 0.7116 - val_accuracy: 0.5123 - lr: 0.0010 - 7s/epoch - 60ms/step\n",
            "Epoch 7/200\n",
            "121/121 - 7s - loss: 0.6686 - accuracy: 0.6119 - val_loss: 0.7140 - val_accuracy: 0.5123 - lr: 0.0010 - 7s/epoch - 60ms/step\n",
            "Epoch 8/200\n",
            "121/121 - 7s - loss: 0.6688 - accuracy: 0.6122 - val_loss: 0.7069 - val_accuracy: 0.5129 - lr: 0.0010 - 7s/epoch - 60ms/step\n",
            "Epoch 9/200\n",
            "121/121 - 7s - loss: 0.6686 - accuracy: 0.6116 - val_loss: 0.7118 - val_accuracy: 0.5117 - lr: 0.0010 - 7s/epoch - 60ms/step\n",
            "Epoch 10/200\n",
            "121/121 - 7s - loss: 0.6678 - accuracy: 0.6123 - val_loss: 0.7097 - val_accuracy: 0.5117 - lr: 0.0010 - 7s/epoch - 60ms/step\n",
            "Epoch 11/200\n",
            "121/121 - 7s - loss: 0.6678 - accuracy: 0.6121 - val_loss: 0.7118 - val_accuracy: 0.5123 - lr: 1.0000e-04 - 7s/epoch - 61ms/step\n",
            "Epoch 12/200\n",
            "121/121 - 7s - loss: 0.6679 - accuracy: 0.6114 - val_loss: 0.7107 - val_accuracy: 0.5123 - lr: 1.0000e-04 - 7s/epoch - 60ms/step\n",
            "Epoch 13/200\n",
            "121/121 - 7s - loss: 0.6674 - accuracy: 0.6123 - val_loss: 0.7108 - val_accuracy: 0.5129 - lr: 1.0000e-04 - 7s/epoch - 60ms/step\n",
            "Epoch 14/200\n",
            "121/121 - 7s - loss: 0.6675 - accuracy: 0.6118 - val_loss: 0.7125 - val_accuracy: 0.5123 - lr: 1.0000e-04 - 7s/epoch - 60ms/step\n",
            "Epoch 15/200\n",
            "121/121 - 7s - loss: 0.6673 - accuracy: 0.6115 - val_loss: 0.7123 - val_accuracy: 0.5105 - lr: 1.0000e-04 - 7s/epoch - 60ms/step\n",
            "Epoch 16/200\n",
            "121/121 - 7s - loss: 0.6673 - accuracy: 0.6112 - val_loss: 0.7120 - val_accuracy: 0.5111 - lr: 1.0000e-04 - 7s/epoch - 61ms/step\n",
            "Epoch 17/200\n",
            "121/121 - 7s - loss: 0.6667 - accuracy: 0.6121 - val_loss: 0.7124 - val_accuracy: 0.5117 - lr: 1.0000e-04 - 7s/epoch - 60ms/step\n",
            "Epoch 18/200\n",
            "121/121 - 7s - loss: 0.6667 - accuracy: 0.6123 - val_loss: 0.7112 - val_accuracy: 0.5105 - lr: 1.0000e-04 - 7s/epoch - 60ms/step\n",
            "Epoch 19/200\n",
            "121/121 - 7s - loss: 0.6666 - accuracy: 0.6116 - val_loss: 0.7116 - val_accuracy: 0.5111 - lr: 1.0000e-04 - 7s/epoch - 60ms/step\n",
            "Epoch 20/200\n",
            "121/121 - 7s - loss: 0.6666 - accuracy: 0.6123 - val_loss: 0.7140 - val_accuracy: 0.5105 - lr: 1.0000e-04 - 7s/epoch - 60ms/step\n",
            "Epoch 21/200\n",
            "121/121 - 7s - loss: 0.6663 - accuracy: 0.6126 - val_loss: 0.7137 - val_accuracy: 0.5105 - lr: 1.0000e-05 - 7s/epoch - 61ms/step\n",
            "Epoch 22/200\n",
            "121/121 - 7s - loss: 0.6660 - accuracy: 0.6122 - val_loss: 0.7136 - val_accuracy: 0.5111 - lr: 1.0000e-05 - 7s/epoch - 61ms/step\n",
            "Epoch 23/200\n",
            "121/121 - 7s - loss: 0.6665 - accuracy: 0.6121 - val_loss: 0.7133 - val_accuracy: 0.5111 - lr: 1.0000e-05 - 7s/epoch - 60ms/step\n",
            "Epoch 24/200\n",
            "121/121 - 7s - loss: 0.6658 - accuracy: 0.6128 - val_loss: 0.7134 - val_accuracy: 0.5111 - lr: 1.0000e-05 - 7s/epoch - 60ms/step\n",
            "Epoch 25/200\n",
            "121/121 - 7s - loss: 0.6660 - accuracy: 0.6121 - val_loss: 0.7133 - val_accuracy: 0.5105 - lr: 1.0000e-05 - 7s/epoch - 60ms/step\n",
            "Epoch 26/200\n",
            "121/121 - 7s - loss: 0.6663 - accuracy: 0.6119 - val_loss: 0.7131 - val_accuracy: 0.5105 - lr: 1.0000e-05 - 7s/epoch - 60ms/step\n",
            "Epoch 27/200\n",
            "121/121 - 7s - loss: 0.6657 - accuracy: 0.6123 - val_loss: 0.7131 - val_accuracy: 0.5105 - lr: 1.0000e-05 - 7s/epoch - 60ms/step\n",
            "Epoch 28/200\n",
            "121/121 - 7s - loss: 0.6659 - accuracy: 0.6128 - val_loss: 0.7131 - val_accuracy: 0.5105 - lr: 1.0000e-05 - 7s/epoch - 60ms/step\n",
            "Epoch 29/200\n",
            "121/121 - 7s - loss: 0.6658 - accuracy: 0.6124 - val_loss: 0.7130 - val_accuracy: 0.5105 - lr: 1.0000e-05 - 7s/epoch - 60ms/step\n",
            "Epoch 30/200\n",
            "121/121 - 7s - loss: 0.6656 - accuracy: 0.6119 - val_loss: 0.7129 - val_accuracy: 0.5105 - lr: 1.0000e-05 - 7s/epoch - 60ms/step\n",
            "Epoch 31/200\n",
            "121/121 - 7s - loss: 0.6657 - accuracy: 0.6113 - val_loss: 0.7129 - val_accuracy: 0.5105 - lr: 1.0000e-06 - 7s/epoch - 60ms/step\n",
            "Epoch 32/200\n",
            "121/121 - 7s - loss: 0.6652 - accuracy: 0.6111 - val_loss: 0.7129 - val_accuracy: 0.5105 - lr: 1.0000e-06 - 7s/epoch - 60ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSQ54ZuErTun"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}